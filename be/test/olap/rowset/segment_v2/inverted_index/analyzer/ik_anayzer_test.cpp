// Licensed to the Apache Software Foundation (ASF) under one
// or more contributor license agreements.  See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.  The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.  You may obtain a copy of the License at
//
//   http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

#include <gtest/gtest.h>

#include <fstream>
#include <memory>
#include <sstream>
#include <filesystem>

#include "olap/rowset/segment_v2/inverted_index/analyzer/ik/IKAnalyzer.h"
#include "olap/rowset/segment_v2/inverted_index/analyzer/ik/IKAnalyzer.h"
#include "CLucene/analysis/LanguageBasedAnalyzer.h"
using namespace lucene::analysis;

namespace doris::segment_v2 {

class IKTokenizerTest : public ::testing::Test {
protected:
    void tokenize(const std::string& s, std::vector<std::string>& datas, bool isSmart) {
        try {
            IKAnalyzer analyzer;
            analyzer.initDict("./be/dict/ik");
            analyzer.setMode(isSmart);
            analyzer.set_lowercase(true);

            lucene::util::SStringReader<char> reader;
            reader.init(s.data(), s.size(), false);

            std::unique_ptr<IKTokenizer> tokenizer;
            tokenizer.reset((IKTokenizer*)analyzer.tokenStream(L"", &reader));

            Token t;
            while (tokenizer->next(&t)) {
                std::string term(t.termBuffer<char>(), t.termLength<char>());
                datas.emplace_back(term);
            }
        } catch (CLuceneError& e) {
            std::cout << e.what() << std::endl;
            throw;
        }
    }
};

TEST_F(IKTokenizerTest, TestIKTokenizer) {
    std::vector<std::string> datas;

    // smart mode
    std::string Text1 = "ä¸­åäººæ°‘å…±å’Œå›½å›½æ­Œ";
    tokenize(Text1, datas, false);
    ASSERT_EQ(datas.size(), 10);
    datas.clear();

    // max_word mode
    tokenize(Text1, datas, true);
    ASSERT_EQ(datas.size(), 2);
    datas.clear();

    std::string Text2 = "äººæ°‘å¯ä»¥å¾—åˆ°æ›´å¤šå®æƒ ";
    tokenize(Text2, datas, false);
    ASSERT_EQ(datas.size(), 5);
    datas.clear();

    // max_word mode
    tokenize(Text2, datas, true);
    ASSERT_EQ(datas.size(), 5);
    datas.clear();

    std::string Text3 = "ä¸­å›½äººæ°‘é“¶è¡Œ";
    tokenize(Text3, datas, false);
    ASSERT_EQ(datas.size(), 8);
    datas.clear();

    // max_word mode
    tokenize(Text3, datas, true);
    ASSERT_EQ(datas.size(), 1);
    datas.clear();
}

TEST_F(IKTokenizerTest, TestIKRareTokenizer) {
    std::vector<std::string> datas;

    std::string Text = "è©ğªœ®é¾Ÿé¾™éºŸå‡¤å‡¤";
    tokenize(Text, datas, true);
    ASSERT_EQ(datas.size(), 4);
    std::vector<std::string> result = {"è©", "ğªœ®", "é¾Ÿé¾™éºŸå‡¤", "å‡¤"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result[i]);
    }
}

TEST_F(IKTokenizerTest, TestIKSmartModeTokenizer) {
    std::vector<std::string> datas;

    std::string Text1 = "æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦";
    tokenize(Text1, datas, true);
    ASSERT_EQ(datas.size(), 4);
    std::vector<std::string> result1 = {"æˆ‘", "æ¥åˆ°", "åŒ—äº¬", "æ¸…åå¤§å­¦"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result1[i]);
    }
    datas.clear();

    std::string Text2 = "ä¸­å›½çš„ç§‘æŠ€å‘å±•åœ¨ä¸–ç•Œä¸Šå¤„äºé¢†å…ˆ";
    tokenize(Text2, datas, true);
    ASSERT_EQ(datas.size(), 7);
    std::vector<std::string> result2 = {"ä¸­å›½", "çš„", "ç§‘æŠ€", "å‘å±•", "åœ¨ä¸–ç•Œä¸Š", "å¤„äº", "é¢†å…ˆ"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result2[i]);
    }
    datas.clear();

}

TEST_F(IKTokenizerTest, TestIKMaxWordModeTokenizer) {
    std::vector<std::string> datas;

    std::string Text1 = "æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦";
    tokenize(Text1, datas, false);
    ASSERT_EQ(datas.size(), 6);
    std::vector<std::string> result1 = {"æˆ‘", "æ¥åˆ°", "åŒ—äº¬", "æ¸…åå¤§å­¦", "æ¸…å", "å¤§å­¦"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result1[i]);
    }
    datas.clear();

    std::string Text2 = "ä¸­å›½çš„ç§‘æŠ€å‘å±•åœ¨ä¸–ç•Œä¸Šå¤„äºé¢†å…ˆ";
    tokenize(Text2, datas, false);
    ASSERT_EQ(datas.size(), 11);
    std::vector<std::string> result2 = {"ä¸­å›½", "çš„", "ç§‘æŠ€", "å‘å±•", "åœ¨ä¸–ç•Œä¸Š", "åœ¨ä¸–", "ä¸–ç•Œä¸Š", "ä¸–ç•Œ", "ä¸Š", "å¤„äº", "é¢†å…ˆ"};
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], result2[i]);
    }
    datas.clear();
}

TEST_F(IKTokenizerTest, TestEmptyInput) {
    std::vector<std::string> datas;
    std::string emptyText = "";
    tokenize(emptyText, datas, true);
    ASSERT_EQ(datas.size(), 0);
}

TEST_F(IKTokenizerTest, TestSingleByteInput) {
    std::vector<std::string> datas;
    std::string singleByteText = "b";
    tokenize(singleByteText, datas, true);
    ASSERT_EQ(datas.size(), 1);
    ASSERT_EQ(datas[0], "b");
}

TEST_F(IKTokenizerTest, TestMultiByteCharBoundary) {
    std::vector<std::string> datas;
    // ä½¿ç”¨ä¸€ä¸ªåŒ…å«å¤šå­—èŠ‚å­—ç¬¦çš„æ–‡æœ¬ï¼Œç¡®ä¿å­—ç¬¦è¾¹ç•Œå¤„ç†æ­£ç¡®
    std::string multiByteText = "ä½ å¥½ä¸–ç•Œ";
    tokenize(multiByteText, datas, true);
    ASSERT_EQ(datas.size(), 2);
    ASSERT_EQ(datas[0], "ä½ å¥½");
    ASSERT_EQ(datas[1], "ä¸–ç•Œ");
}

TEST_F(IKTokenizerTest, TestLargeInput) {
    std::vector<std::string> datas;
    // åˆ›å»ºä¸€ä¸ªå¤§äºBUFF_SIZE(4096)çš„è¾“å…¥ï¼Œæµ‹è¯•ç¼“å†²åŒºé‡æ–°å¡«å……
    std::string largeText;
    for (int i = 0; i < 1000; i++) {
        largeText += "ä¸­å›½çš„ç§‘æŠ€å‘å±•åœ¨ä¸–ç•Œä¸Šå¤„äºé¢†å…ˆ";
    }
    tokenize(largeText, datas, true);
    // éªŒè¯åˆ†è¯ç»“æœæ•°é‡åº”è¯¥æ˜¯7000ï¼ˆæ¯ä¸ªçŸ­è¯­åˆ†æˆ7ä¸ªè¯ï¼Œé‡å¤1000æ¬¡ï¼‰
    ASSERT_EQ(datas.size(), 7000);
}

TEST_F(IKTokenizerTest, TestBufferExhaustCritical) {
    std::vector<std::string> datas;
    // åˆ›å»ºä¸€ä¸ªæ¥è¿‘BUFF_EXHAUST_CRITICAL(100)çš„è¾“å…¥
    std::string criticalText;
    for (int i = 0; i < 95; i++) {
        criticalText += "çš„";
    }
    tokenize(criticalText, datas, true);
    ASSERT_EQ(datas.size(), 95);
}

TEST_F(IKTokenizerTest, TestMixedLanguageInput) {
    std::vector<std::string> datas;
    // æµ‹è¯•æ··åˆè¯­è¨€è¾“å…¥ï¼ˆä¸­è‹±æ–‡ã€æ•°å­—ã€ç¬¦å·ç­‰ï¼‰
    std::string mixedText = "Dorisæ˜¯ä¸€ä¸ªç°ä»£åŒ–çš„MPPåˆ†æå‹æ•°æ®åº“ï¼Œå¯ä»¥å¤„ç†PBçº§åˆ«çš„æ•°æ®ï¼Œæ”¯æŒSQL92å’ŒSQL99ã€‚";
    tokenize(mixedText, datas, true);
    // éªŒè¯åˆ†è¯ç»“æœåŒ…å«è‹±æ–‡å•è¯å’Œä¸­æ–‡è¯è¯­
    std::vector<std::string> expectedTokens = {"doris", "æ˜¯", "ä¸€ä¸ª", "ç°ä»£åŒ–", "çš„", "mpp", "åˆ†æ", "å‹", "æ•°æ®åº“", "å¯ä»¥", "å¤„ç†", "pb", "çº§", "åˆ«çš„", "æ•°æ®", "æ”¯æŒ", "sql92", "å’Œ", "sql99"};
    ASSERT_EQ(datas.size(), expectedTokens.size());
    for (size_t i = 0; i < datas.size(); i++) {
        ASSERT_EQ(datas[i], expectedTokens[i]);
    }
}

TEST_F(IKTokenizerTest, TestSpecialCharacters) {
    std::vector<std::string> datas;
    // æµ‹è¯•ç‰¹æ®Šå­—ç¬¦å’Œç½•è§å­—ç¬¦çš„å¤„ç†
    std::string specialText = "ğŸ˜ŠğŸš€ğŸ‘æµ‹è¯•ç‰¹æ®Šç¬¦å·ï¼š@#Â¥%â€¦â€¦&*ï¼ˆï¼‰";
    tokenize(specialText, datas, true);
    // éªŒè¯ç‰¹æ®Šå­—ç¬¦çš„å¤„ç†ç»“æœ
    ASSERT_GT(datas.size(), 0);
}
}
